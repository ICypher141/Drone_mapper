{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique mask values before correction: [ 0 90]\n",
      "Warning: Unable to load mask Class_Segmentation\\10.jpeg\n",
      "Unique mask values before correction: [  0 170]\n",
      "Unique mask values before correction: [  0 170]\n",
      "Unique mask values before correction: [  0 170]\n",
      "Unique mask values before correction: [  0 170]\n",
      "Warning: Unable to load mask Class_Segmentation\\3.jpeg\n",
      "Unique mask values before correction: [  0 170]\n",
      "Unique mask values before correction: [  0 170]\n",
      "Unique mask values before correction: [ 0 90]\n",
      "Unique mask values before correction: [  0 170]\n",
      "Unique mask values before correction: [  0 170]\n",
      "Unique mask values before correction: [ 0 90]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_agricultural_data(image_dir, mask_dir, target_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Load images and segmentation masks for agricultural segmentation.\n",
    "    \n",
    "    Args:\n",
    "        image_dir (str): Directory containing aerial images.\n",
    "        mask_dir (str): Directory containing corresponding segmentation masks.\n",
    "        target_size (tuple): Desired size of the images and masks (default: 256x256).\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: Preprocessed images and masks.\n",
    "    \"\"\"\n",
    "    images, masks = [], []\n",
    "    for file_name in os.listdir(image_dir):\n",
    "        # Image and mask paths\n",
    "        image_path = os.path.join(image_dir, file_name)\n",
    "        mask_path = os.path.join(mask_dir, file_name.replace('.jpg', '.png'))  # Match extensions\n",
    "\n",
    "        # Load and resize image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Warning: Unable to load image {image_path}\")\n",
    "            continue\n",
    "        image = cv2.resize(image, target_size) / 255.0  # Normalize image\n",
    "\n",
    "        # Load and resize mask\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load mask as grayscale\n",
    "        if mask is None:\n",
    "            print(f\"Warning: Unable to load mask {mask_path}\")\n",
    "            continue\n",
    "        mask = cv2.resize(mask, target_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Debug: Check unique mask values\n",
    "        print(f\"Unique mask values before correction: {np.unique(mask)}\")\n",
    "\n",
    "        # Handle out-of-range values\n",
    "        mask[mask >= 4] = 0  # Replace invalid labels with background\n",
    "\n",
    "        # Convert to one-hot encoding\n",
    "        mask = tf.keras.utils.to_categorical(mask, num_classes=4)\n",
    "\n",
    "        images.append(image)\n",
    "        masks.append(mask)\n",
    "\n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "# Set dataset paths\n",
    "image_dir = \"Plantations_Segmentation/img\"\n",
    "mask_dir = \"Class_Segmentation\"\n",
    "\n",
    "# Load training and validation data\n",
    "train_images, train_masks = load_agricultural_data(image_dir, mask_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique mask values before correction: [ 0 90]\n",
      "Warning: Unable to load mask Class_Segmentation\\10.jpeg\n",
      "Unique mask values before correction: [  0 170]\n",
      "Unique mask values before correction: [  0 170]\n",
      "Unique mask values before correction: [  0 170]\n",
      "Unique mask values before correction: [  0 170]\n",
      "Warning: Unable to load mask Class_Segmentation\\3.jpeg\n",
      "Unique mask values before correction: [  0 170]\n",
      "Unique mask values before correction: [  0 170]\n",
      "Unique mask values before correction: [ 0 90]\n",
      "Unique mask values before correction: [  0 170]\n",
      "Unique mask values before correction: [  0 170]\n",
      "Unique mask values before correction: [ 0 90]\n",
      "Epoch 1/7\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 356ms/step - accuracy: 0.5247 - loss: 1.3104 - val_accuracy: 1.0000 - val_loss: 0.3986\n",
      "Epoch 2/7\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 289ms/step - accuracy: 1.0000 - loss: 0.2342 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
      "Epoch 3/7\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272ms/step - accuracy: 1.0000 - loss: 5.5064e-04 - val_accuracy: 1.0000 - val_loss: 1.8452e-05\n",
      "Epoch 4/7\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 298ms/step - accuracy: 1.0000 - loss: 2.5469e-05 - val_accuracy: 1.0000 - val_loss: 8.4162e-07\n",
      "Epoch 5/7\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268ms/step - accuracy: 1.0000 - loss: 2.2338e-06 - val_accuracy: 1.0000 - val_loss: 7.7750e-08\n",
      "Epoch 6/7\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 294ms/step - accuracy: 1.0000 - loss: 1.1116e-07 - val_accuracy: 1.0000 - val_loss: 1.2956e-08\n",
      "Epoch 7/7\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 291ms/step - accuracy: 1.0000 - loss: 9.4283e-08 - val_accuracy: 1.0000 - val_loss: 3.3267e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "\n",
    "def build_simple_unet(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Build a simplified U-Net model for semantic segmentation.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Shape of input images, e.g., (256, 256, 3).\n",
    "        num_classes (int): Number of output classes for segmentation.\n",
    "\n",
    "    Returns:\n",
    "        tensorflow.keras.Model: Simplified U-Net model.\n",
    "    \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # Downsampling\n",
    "    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Bottleneck\n",
    "    c3 = Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n",
    "\n",
    "    # Upsampling\n",
    "    u4 = UpSampling2D((2, 2))(c3)\n",
    "    u4 = concatenate([u4, c2])\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same')(u4)\n",
    "\n",
    "    u5 = UpSampling2D((2, 2))(c4)\n",
    "    u5 = concatenate([u5, c1])\n",
    "    c5 = Conv2D(32, (3, 3), activation='relu', padding='same')(u5)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(c5)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Build and compile the model\n",
    "input_shape = (256, 256, 3)\n",
    "num_classes = 4  # Replace with the number of your classes\n",
    "model = build_simple_unet(input_shape, num_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Load data using the previously defined function\n",
    "image_dir = \"Plantations_Segmentation/img\"\n",
    "mask_dir = \"Class_Segmentation\"\n",
    "train_images, train_masks = load_agricultural_data(image_dir, mask_dir)\n",
    "\n",
    "# Placeholder validation split\n",
    "split_idx = int(0.8 * len(train_images))\n",
    "val_images, val_masks = train_images[split_idx:], train_masks[split_idx:]\n",
    "train_images, train_masks = train_images[:split_idx], train_masks[:split_idx]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_images, train_masks,\n",
    "    validation_data=(val_images, val_masks),\n",
    "    batch_size=2,\n",
    "    epochs=7\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('simple_drone_segmentation_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
